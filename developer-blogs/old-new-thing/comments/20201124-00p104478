Jonathan Harston  December 2, 2020


  0  Collapse this comment
Copy link
Years‚Äôn‚Äôyears ago I wrote a disk optimiser that would put the files in a directory in adjacent sectors to the ones occupied by the directory. The rationale being that after scanning the directory for the file‚Äôs information there would be minimal steps from the directory to the sectors holding the files.





aidtopia  December 1, 2020


  0  Collapse this comment
Copy link
Having more sectors per track on the outer cylinders is called zone bit recording.  Rather than having to compute the sectors per track for a given cylinder, you divide the cylinders into "zones" and determine the number of sectors per track based on what zone it's in.
While it seems like an obvious and straight-forward optimization for data density, it can add a surprising amount of complexity.
For example, when you change zones, you not only need to know the number of sectors per track, you might have to adjust the clocking registers used to manage the frequency of the write...Read moreHaving more sectors per track on the outer cylinders is called zone bit recording.  Rather than having to compute the sectors per track for a given cylinder, you divide the cylinders into ‚Äúzones‚Äù and determine the number of sectors per track based on what zone it‚Äôs in.
While it seems like an obvious and straight-forward optimization for data density, it can add a surprising amount of complexity.
For example, when you change zones, you not only need to know the number of sectors per track, you might have to adjust the clocking registers used to manage the frequency of the write heads.
Also, you usually put your spare sectors at the end of each zone rather than at the end of the drive.  So if you encounter a bad sector, you don‚Äôt have to seek very far to get to a spare, nor do you have to adjust the read-write frequencies.  That is, until you‚Äôve used up all the spares in your zone.  Then you need a way to select a spare from an adjacent zone and have a way to maintain that mapping.
Putting files on the outer cylinders is great for high bandwidth transfers, but they typically also have latency (because of the extra seek time from the ‚Äúcenter‚Äù of the drive where, ideally, most of the hot files should be).  So it‚Äôs perfect for rare things that need to bring in a lot of data at once, like boot-time operations.
Read less







Goran Mitrovic  November 27, 2020


  0  Collapse this comment
Copy link
I have a defrag question ‚Äì why is mrt.exe always so heavily fragmented?





skSdnW  November 25, 2020


  0  Collapse this comment
Copy link
I never understood why Windows did not do more of this, specifically move stuff like .msi and service pack backup files to the slow part of the disk.
It would be cool if a API was added so you could give a hint to a file handle if the file storage area should be normal or slow.





Aged .Net Guy  November 25, 2020


  0  Collapse this comment
Copy link
Within the Windows team that sounds like a potentially useful feature.
But I have a hard time imagining any PHB at, say, Litware, agreeing that any of the files in their product should volunteer for second class status performance-wise versus competing products. It‚Äôs another variation on the taller ladders & walls situation.




Spiro Trikaliotis  November 25, 2020


  0  Collapse this comment
Copy link
But you should also mention that this layout (lower-numbered block numbers are faster) is not contractual. So, one should not rely on this implementation detail. üòâ
There is nothing preventing a disc manufacturer from arranging it in the opposite order, that is, the highest-numbered block numbers would be the fastest then.
Or they can lay it out as follows: First, head 0 is filled completely. Then, head 1 is filled completely. Then, head 2, and so on.
This way, the block number vs. performance graph will look mostly like sawteeth.
This has been done before, so, this is not completely made up.





George Jansen  November 25, 2020


  0  Collapse this comment
Copy link
Back in the 1980s, I worked for a company that built systems on Data General minis. There was one model of disk, I think the 73 MB, for which the 16-bit OS had to be installed very close to the outer edge, or it wouldn‚Äôt boot. (On all disk models, the OS had to be a contiguous file, though the overlay file didn‚Äôt. ) I remember that prudent operators would sometimes lay down extra copies there to reserve space for upgrades.





Jonathan Harston  December 2, 2020


  0  Collapse this comment
Copy link
That rings a bell. I think it was something like the filesystem used 16-bit block numbers, but the operating system loader was optimised to use 8-bit numbers, so it all had to live in the first 256 blocks, 0000 to 00FF.




Neil Rashbrook  November 25, 2020


  0  Collapse this comment
Copy link
And here was me thinking that the heads must be at cylinder 0 in order to read the boot sector, so why not make use of that and put as much other boot-time data there as possible?





smf  November 25, 2020


  0  Collapse this comment
Copy link
The boot is only at cylinder 0 because it is easier if you always boot from a known place and everything has to have a 0.
But the hard drive controller manufacturers decided which end of the disk was 0.
They probably made the same decision, that 0 should be the faster area of the disk.
If you start allocating the fastest then it feels really fast when you first install, but gets slower as you install all the files (that you‚Äôll end up using more).




smf  November 24, 2020


  0  Collapse this comment
Copy link
I rarely reboot. Is optimizing for boot times the best use of that resource?





aidtopia  December 1, 2020


  0  Collapse this comment
Copy link
Putting the boot information on the outer cylinders isn‚Äôt (only) about optimizing for boot times.  The outer cylinders typically require the farthest seeks, so the latency is bad.  You don‚Äôt want your hot files there.
Putting the boot files out at the edge is good for boot times (where the bandwidth bonus exceeds the seek latency you‚Äôre probably already paying for), but it also means those big files won‚Äôt be taking up valuable real estate in the ‚Äúmiddle‚Äù of the drive, where you want the smaller files you hit frequently.





Florian Schneidereit  November 25, 2020


  0  Collapse this comment
Copy link
I always shutdown and reboot my PCs because it‚Äôs a waste of energy to keep unused devices powered on or in stand-by. I mean, if you sum it up, we literally need several gigawatt power plants just for convenience and I think this is a no go. With a SSD, or even a modern HDD, boot times are neglectable today anyways. For example, the monitor of my desktop PC is not slow to become ready but it takes longer to power up than the boot process.





Flux  November 24, 2020


  0  Collapse this comment
Copy link
Well, bless your heart. I myself never reboot my coworker‚Äôs computer.





smf  November 25, 2020


  0  Collapse this comment
Copy link
What is your point?
The fastest area of the disk should be used for things you do the most. If the thing you do the most is reboot then sure, it seems like a great idea. If not then it seems like a premature optimization.
Nowadays of course your coworkers should get an SSD.