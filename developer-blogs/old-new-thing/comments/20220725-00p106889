Robin Hoffmann  July 25, 2022


  0  Collapse this comment
Copy link
Could they have made the bit fields variable, depending on “op”? For example, as you said, “d” wasn’t required for register-to-register copies.





Antonio Rodríguez  July 25, 2022
· Edited

  0  Collapse this comment
Copy link
Theoretically, it could have been possible. But in practice, there would have been two problems.
First, encoding parameters in the same way for several opcodes allows you to use the same decoding/fetching circuitry for most move and ALU instructions. In those days, a lower transistor count would mean greater product yields and lower prices. That's why the 6502 totally eclipsed Motorola's 6800: it had the lowest transistor count, and thus on launch it was also the cheapest processor, by a wide margin.
Second, having different addressing modes available in different instructions is a very bad idea when you hand write assembly code,...Read moreTheoretically, it could have been possible. But in practice, there would have been two problems.
First, encoding parameters in the same way for several opcodes allows you to use the same decoding/fetching circuitry for most move and ALU instructions. In those days, a lower transistor count would mean greater product yields and lower prices. That’s why the 6502 totally eclipsed Motorola’s 6800: it had the lowest transistor count, and thus on launch it was also the cheapest processor, by a wide margin.
Second, having different addressing modes available in different instructions is a very bad idea when you hand write assembly code, which was usual at the time. The 6502 suffered this, but it was almost bearable because its instruction set was small and there weren’t many exceptions, so you learned them after some time. The 8086 had a much larger instruction set, which would have made it painful. In fact, when the 68000 arrived, it was usually praised for the orthogonality of its instruction set (compared with that of the 8086, that is, which in itself was much more uniform than the 8080 it replaced).
Read less







Simon Farnsworth  July 25, 2022


  0  Collapse this comment
Copy link
As a historical note, it's worth remembering that the iAPX 432 was the design that was getting all the attention at Intel; it started before the 8086 did.
The 8086 was meant as a stop-gap to allow existing 8080 and 8085 customers to move into the 16 bit world easily, while the iAPX 432 was going to be the next big leap. And the iAPX 432 had variable length instructions - 6 bits for the shortest, 321 bits for the longest - that didn't need to be byte aligned, so it didn't have this specific issue.
So one of the issues that...Read moreAs a historical note, it’s worth remembering that the iAPX 432 was the design that was getting all the attention at Intel; it started before the 8086 did.
The 8086 was meant as a stop-gap to allow existing 8080 and 8085 customers to move into the 16 bit world easily, while the iAPX 432 was going to be the next big leap. And the iAPX 432 had variable length instructions – 6 bits for the shortest, 321 bits for the longest – that didn’t need to be byte aligned, so it didn’t have this specific issue.
So one of the issues that you’d have trying to get the 8086 to use more complex decode is that it’s the stop-gap; the thing you’re releasing not because it’s meant to take over the world (that’s the iAPX 432), but the thing you’re releasing so that the world will wait for the next big thing.
Read less







Antonio Rodríguez  July 25, 2022


  0  Collapse this comment
Copy link
Well, the iAPX failed precisely because it was too complex for the technology of the time. It had to be broken in two ICs (ALU and control unit), simply because they couldn’t put enough transistors in a single state-of-the-art die. That drove the costs through the ceiling and hindered the communication speed between both units, which resulted in a system that was incompatible, very expensive, and just a hair more powerful than the 8086 (and slower than Motorola’s 68000). The iAPX was, in many ways, decades ahead of its time – and also decades ahead of the technology needed to build it.





Simon Farnsworth  July 26, 2022


  0  Collapse this comment
Copy link
And with 20/20 hindsight, if Intel had known the iAPX 432 was going to fail, they might have paid more attention to making the 8086 a better chip.
That might well have gone badly for the 8086/8088, because one of the reasons the 8086/8088 succeeded is that final decision making on what stayed in the architecture and what didn't was down to a single person, who could keep the shape of the entire design in their head; in contrast the 8800 or iAPX 432 had design committees responsible for it, and a badly chosen feature could remain alive for months until...Read moreAnd with 20/20 hindsight, if Intel had known the iAPX 432 was going to fail, they might have paid more attention to making the 8086 a better chip.
That might well have gone badly for the 8086/8088, because one of the reasons the 8086/8088 succeeded is that final decision making on what stayed in the architecture and what didn’t was down to a single person, who could keep the shape of the entire design in their head; in contrast the 8800 or iAPX 432 had design committees responsible for it, and a badly chosen feature could remain alive for months until the committee could be persuaded to ditch it.
I also question whether the iAPX 432 was ahead of its time – it could have been, but it could also have been like the Itanium design. Itanium failed in large part because it assumed that OoOE could not scale nicely, but memory throughput, cache size, and cache throughput would scale. Instead, OoOE scaled nicely, such that all the compiler improvements done for Itanium benefited OoOE CPUs just as much as they did EPIC CPUs, and both memory and caches stalled out relative to the expectations of the Itanium design committee.
I believe that iAPX 432 was similar to Itanium in this regard; it made assumptions about the future of technology that turned out to be wrong (among other things, it assumed that we’d rewrite all software in Ada instead of continuing with Pascal or C), and that it would have been a failure even with better manufacturing technology as a result.
Read less







Pete Nelson  July 25, 2022


  0  Collapse this comment
Copy link
Kind of like how IA-64 was going to take over the 64-bit world, but AMD64 came in with an easier migration path and ate IA-64’s lunch…