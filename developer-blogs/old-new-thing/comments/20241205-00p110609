Kevin Norris  December 9, 2024


  0  Collapse this comment
Copy link
I suspect that questions of this nature arise from a faulty generalization of otherwise sound principles. If you are working in a highly concurrent system, the usual rule of thumb is to avoid doing things one at a time unless the operation is inherently synchronous and can't be parallelized, because serial execution imposes a fairly significant bottleneck. That is, you're only making forward progress on one thing at a time, instead of N at a time, so you are N / 1 = N times slower than optimal. It's possibly even worse than that, because there may be a subsequent...Read moreI suspect that questions of this nature arise from a faulty generalization of otherwise sound principles. If you are working in a highly concurrent system, the usual rule of thumb is to avoid doing things one at a time unless the operation is inherently synchronous and can’t be parallelized, because serial execution imposes a fairly significant bottleneck. That is, you’re only making forward progress on one thing at a time, instead of N at a time, so you are N / 1 = N times slower than optimal. It’s possibly even worse than that, because there may be a subsequent step that consumes your output, that step cannot run in parallel if you only give it one output at a time, and this continues all the way down the pipeline until we hit a reduce operation, so you may effectively serialize large portions of the overall computation, rather than just the individual step we’re considering.
That doesn’t apply to waiting, because waiting makes no forward progress in the first place, so (as you say) we’re only incurring overhead from context switches and the like, which is a lot cheaper than a full-blown serialization bottleneck. But I believe that some programmers simply assume that “one at a time = bad” instead of thinking about why it is bad.
Read less