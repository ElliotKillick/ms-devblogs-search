Nick  April 8, 2024


  0  Collapse this comment
Copy link
Since I can‚Äôt reply to an unapproved comment, just wondering why my earlier comment from April 1 was never approved.  I‚Äôm sure you‚Äôre busy, but otherwise, did I do something wrong to cause it to get stuck (the YouTube link)?





Stephan Leclercq  April 5, 2024


  0  Collapse this comment
Copy link
And then there was COBOL‚Ä¶ You can‚Äôt pretend you understand spaghetti code unless you‚Äôve written code like this at least once in your lifetime:
    PERFORM PARAGRAPH-A THROUGH PARAGRAPH-Z.
    ALTER PARAGRAPH-B TO PROCEED TO PARAGRAPH-D.





Xuejun Yang  April 4, 2024


  0  Collapse this comment
Copy link
Hey, this is exactly what we did for this paper: https://users.cs.utah.edu/~regehr/papers/lctes062-yang.pdf. By translating function calls to a bunch of goto statements. Our research shows it makes sense for embedded applications when the memory is a scarce resource.





Richard Deeming  April 4, 2024


  0  Collapse this comment
Copy link
My first thought on reading the title was of using GOSUB and RETURN on a ZX Spectrum. üôÇ





Yuri Khan  April 4, 2024


  0  Collapse this comment
Copy link
Yes and no. Early BASICs had a return address stack but no argument/locals stack. (No subroutine arguments nor local variables as such.) I once did a recursive Sierpinski triangle in those restrictions, simulating locals stack over a few integer arrays. Luckily, a maximum recursion depth of about 6 or 8 was quite sufficient for that time‚Äôs screen pixel counts.




Mark Yagnatinsky  April 3, 2024


  0  Collapse this comment
Copy link
Aside from not supporting recursion, this sounds like it would not cope well with function pointers, right?





Raymond Chen Author
April 3, 2024


  1  Collapse this comment
Copy link
Programming languages of the day generally didn‚Äôt have the concept of ‚Äúfunction pointers‚Äù. But if they did, you can make function pointers be, say, a pointer to a structure that has the code address (read-only), the return address (read-write), and the parameters (read-write).





Joshua Hudson  April 13, 2024


  0  Collapse this comment
Copy link
Register calling conventions also date back pretty far; in the case of the return address being at the start of the function, it would work; however with old processors you will find yourself limited to functions that take one or two arguments.



aidtopia  April 3, 2024


  0  Collapse this comment
Copy link
It's not just the ancient world!  There are still domains today where dynamic memory allocation is not allowed and you have to guarantee an upper bound on the stack size (which often implies no recursion).  If your embedded system is a critical safety component, you probably have to conform to guidelines, like MISRA, that prohibit such perilous luxuries.
"[J]ust [exit] the program with a fatal error." Ha! not unless you can guarantee the program...Read moreIt‚Äôs not just the ancient world!  There are still domains today where dynamic memory allocation is not allowed and you have to guarantee an upper bound on the stack size (which often implies no recursion).  If your embedded system is a critical safety component, you probably have to conform to guidelines, like MISRA, that prohibit such perilous luxuries.
‚Äú[J]ust [exit] the program with a fatal error.‚Äù Ha! not unless you can guarantee the program will be immediately restarted in a known good state. üòâ
Read less







Á¥ÖÊ®ìÈçÆ  April 2, 2024
¬∑ Edited

  0  Collapse this comment
Copy link
Stackless coroutines in C++ support inline allocation, if the compiler can prove lifetime shenanigans, can see the definition of the callee coroutine (so it knows how many bytes its coroutine frame consumes) and, of course, if the call isn't recursive. This process can then continue on for the transitive callees of the callee, and the end result is a big, fixed size contiguous buffer that's just big enough to provide memory for the execution of...Read moreStackless coroutines in C++ support inline allocation, if the compiler can prove lifetime shenanigans, can see the definition of the callee coroutine (so it knows how many bytes its coroutine frame consumes) and, of course, if the call isn‚Äôt recursive. This process can then continue on for the transitive callees of the callee, and the end result is a big, fixed size contiguous buffer that‚Äôs just big enough to provide memory for the execution of the entire coroutine call hierarchy. It‚Äôs the compiler taking code that you wrote assuming the omnipresence of the stack, analyzing it, and emitting code in a way as if you coded like the pre-stack programmers did.
Note how the compiler doing this optimization has nothing to do with coroutines; because most functions used in programming do not recurse directly or indirectly, the compiler could, in theory, compile such functions into machine code that operated within a static blob of scratch memory, using a special ‚Äústackless‚Äù calling convention that prohibited accessing the stack pointer in any way; and if you launched a thread with one of those stackless functions as its code, then the thread wouldn‚Äôt need to have a stack (well it needs, because there are devils such as interrupts, APCs, POSIX signals, etc, but in embedded programming there‚Äôs a good chance you‚Äôll one day find yourself creating multiple threads running such stackless functions, and in a world where stackless functions were real, it would save a huge amount of memory to be able to create threads without having to give each thread hundreds of bytes of stack, and while interrupts exist, you now need a stack per processor for handling them, instead of one per thread).
Read less







Neil Rashbrook  April 2, 2024


  1  Collapse this comment
Copy link
Ooh, FORTRAN eventually got recursion? I remember it didn‚Äôt back in the day when I had a course on it. (I think my year was the last year where FORTRAN was a mandatory course for first-year students. It was also the first year that they actually taught C to first year students.)





Dave Gzorple  April 1, 2024


  1  Collapse this comment
Copy link
You didn‚Äôt really need to make up some assembly language, you could have used the most famous instruction set that did this, and one which is still used today, the IBM 360 with balr et al.





Mark out West  April 2, 2024
¬∑ Edited

  0  Collapse this comment
Copy link
Yup.  For reentrancy/recursion/multithreading IBM utilized a "dynamic storage area" protocol in lieu of a hardware stack.  A main memory save area was dynamically allocated and chained together in essentially a doubly linked list that lengthened as the calls deepened.  Each DSA saved the entire register set and the set of passed parameters.  Because the save area was heap allocated, the executable code was considered read-only and thus thread-safe.
Simpler routines simply dumped...Read moreYup.  For reentrancy/recursion/multithreading IBM utilized a ‚Äúdynamic storage area‚Äù protocol in lieu of a hardware stack.  A main memory save area was dynamically allocated and chained together in essentially a doubly linked list that lengthened as the calls deepened.  Each DSA saved the entire register set and the set of passed parameters.  Because the save area was heap allocated, the executable code was considered read-only and thus thread-safe.
Simpler routines simply dumped the register set into a statically allocated save area within the code and restored its contents on exit.
Read less






Ian Boyd  April 1, 2024
¬∑ Edited

  4  Collapse this comment
Copy link
It's still amazing to me that even as recently as Windows 95, which had to run in 4 MB of RAM, that we had to scrimp and save every byte, and every cycle. Where every page fault was a nightmare.
Whereas now the CPU is executing two or three *hundred* instructions ahead, across multple loop iterations, across multiple function calls, and returns. Where a 2 GB address limit now feels confining, and our PCs regularly have...Read moreIt‚Äôs still amazing to me that even as recently as Windows 95, which had to run in 4 MB of RAM, that we had to scrimp and save every byte, and every cycle. Where every page fault was a nightmare.
Whereas now the CPU is executing two or three *hundred* instructions ahead, across multple loop iterations, across multiple function calls, and returns. Where a 2 GB address limit now feels confining, and our PCs regularly have more RAM than we could ever use, so Windows uses the other half of it as a cache (SuperFetch).
Where we don‚Äôt have to have a mini-COM, or declare memory as discardable or not-discardable, or squeeze every byte out of an image with palettes.
And you can have a full 3D recreation of the planet, with every tree and building, running on Javascript inside a browser.
What a wonderous age we live in.
Read less